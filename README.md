# ğŸ•¸ï¸ Ethical AI-Powered Web Scraper (with Tor + Content Filtering)

This project is a powerful yet ethical web scraper built in Python that uses Tor for anonymity, content filtering via LLMs (or static logic), and polite crawling practices. It is designed for testing and research purposes.

## âš™ï¸ Features

- ğŸŒ **Tor-Enabled Anonymous Crawling**  
  Uses `socks5h://127.0.0.1:9050` to route all traffic through Tor.

- ğŸ“œ **robots.txt Override (Test Mode)**  
  Controlled override option to skip robots.txt checks during local testing.

- ğŸ¤– **LLM-Based Relevance Filtering** *(Optional)*  
  Uses [Ollama](https://ollama.com) with a local model (e.g., `mistral`) to determine if a page contains relevant content.

- ğŸ–¼ï¸ **Content Extraction**  
  Extracts clean page titles, paragraphs, image metadata, and table data.

- ğŸ“š **Focused Content Scraping**  
  Can be configured to scrape only relevant sections (like Wikipedia/Fandom â€œContentsâ€).

- ğŸ” **User-Agent Rotation & Polite Delays**  
  Random headers and sleep intervals to reduce detection and stay polite.

---

## ğŸ“ Project Structure
.
â”œâ”€â”€ crawler.py # Main scraper script
â”œâ”€â”€ tor_awake.py # Starts Tor with custom bridges
â”œâ”€â”€ tor_logs.py # For streaming Tor logs
â”œâ”€â”€ test_tor.py # Verifies Tor routing
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ğŸ› ï¸ Requirements

- Python 3.8+
- [Tor](https://www.torproject.org/) (configured to run in background or via `tor_awake.py`)
- [Ollama](https://ollama.com) *(optional, for content filtering)*

Install dependencies:
```bash
pip install -r requirements.txt

Ethics & Disclaimer
This tool is intended strictly for educational and ethical testing purposes.
Always respect robots.txt, Terms of Service, and legal policies of the sites you target.
You are responsible for any misuse.

ğŸ“„ License
Licensed under the MIT License.
